{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HOT\u2019s Infrastructure Modernization: Kubernetes","text":"<p>[!Note] Currently under initial development. </p> <p>Kubernetes @ Humanitarian OpenStreetMap Team (HOT).</p> <p>See the inital proposal for more background.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#required-tools","title":"Required Tools","text":"<ul> <li>AWS CLI</li> <li>OpenTofu</li> <li>kubectl</li> <li>Helm</li> </ul>"},{"location":"#areas-for-further-initial-development","title":"Areas for Further (Initial) Development","text":""},{"location":"#variable-management","title":"Variable Management","text":"<ul> <li>Duplication exists between TF inputs, CI workflows, and local scripts.</li> <li>A tool like https://github.com/helmfile/helmfile may help with sourcing variables by environment.     - A basic version has been added to deploy revision deltas, further templating would be required.</li> <li>As more HOT applications + services are moved to cluster, this will only grow.</li> </ul>"},{"location":"#deployment","title":"Deployment","text":"<ul> <li>Provisioning is currently done in the same workflow (TF, K8s, Helm), mostly as byproduct of initial development phase. Can be further refined.</li> <li>GitOps tools like ArgoCD are in the works</li> <li>Flux Tofu controller may be an analog for base infrastructure (further investigation required).</li> </ul>"},{"location":"#bridging-tf-and-kubernetes","title":"Bridging TF and Kubernetes","text":"<ul> <li>TF-managed information often needs to be referenced on the cluster     - ex: PostgresCluster CRD requires the role ARN authorized for backups. Role and bucket are created in TF.</li> <li>Global cluster resources are provisioned through TF, but argument can be made for their management by K8s. </li> <li>Ideal solution enables cluster resources to reference, mount, inject, etc. TF-managed information with minimal developer intervention.</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"Status Tool Migrated Date Estimate \u2705 OpenAerialMap Sep '25 \u2699\ufe0f FieldTM Jan '26 \u2699\ufe0f Shared Login Jan '26 \u2699\ufe0f DroneTM + ScaleODM Feb '26 ODK, QFieldCloud Mar '26 ChatMap May '26 fAIr June '26 Portal July '26 uMap July '26 HDX Country Exports Sep '26 Export Tool Dec '26 \u274c Tasking Manager Not currently planned \u274c Raw Data API Not currently planned"},{"location":"about/proposal/","title":"HOT\u2019s Infrastructure Modernization: Kubernetes","text":""},{"location":"about/proposal/#requirement","title":"Requirement","text":"<ul> <li>HOT is quite reliant on vendor-specific cloud services for various tools.</li> <li>We rely heavily on donated credits from cloud providers (today, we fully rely on AWS) and have minimal internal funding for infrastructure. This source comes with few guarantees! </li> <li>Ideally we would have a cloud-agnostic approach to deployment, giving the flexibility to deploy anywhere if the case arises (AWS, Azure, Hetzner, on-prem).</li> <li>This is not a small task, and also won\u2019t solve all of our problems, but it would be a great start to migrate as many services as possible to cloud-agnostic approaches, via open tools such as Kubernetes, KEDA (scaling), ArgoCD (GitOps), etc.</li> </ul>"},{"location":"about/proposal/#challenges","title":"Challenges","text":"<ul> <li>Lack of time and resources to dedicate to this as a project - it\u2019s difficult to justify addressing tech debt, when there is the allure of new features and updating software.</li> <li>Lack of expertise in the tech team <ul> <li>Tech lead has previous k8s experience, but little time. </li> <li>Some devs at our partner NAXA have also dabbled with k8s.</li> </ul> </li> <li>Heavy reliance of some tools on vendor-specific services.</li> </ul> <p>In order of most difficult \u2192 least difficult for migration in our 2024 assessment:   - OpenAerialMap (many AWS services / lambda etc).   - fAIr (GPU reliant ML workflows, task scheduling, autoscaling).   - Tasking Manager (autoscaling requirement, large user base).   - Export tool / raw-data-api (task scheduling, redis queue based autoscaling)   - FieldTM &amp; DroneTM</p>"},{"location":"about/proposal/#benefits","title":"Benefits","text":"<ul> <li>We would like to slowly start to become more cloud-agnostic in our deployment approach, making us more resilient to changes in the future.</li> <li>Reduced costs \ud83e\udd1e, with resource utilization spread across a smaller cluster of VMs, instead of many under-utilized standalone VMs, but still able to handle load spikes.</li> </ul>"},{"location":"about/proposal/#proposal","title":"Proposal","text":"<ul> <li>The core of this requirement is to configure a Kubernetes cluster, and start to migrate services into it.</li> <li>This will involve two steps:<ul> <li>Setup of the Kubernetes control plane. Likely EKS, but also open to managing this ourselves with an OS like Talos.</li> <li>Slow migration of services into the cluster, packaging them tools up as Helm charts, and deploying with all additional required components (autoscaling, job queue, etc).</li> </ul> </li> </ul>"},{"location":"about/proposal/#step-1-control-plane-osm-sandbox","title":"Step 1: Control Plane &amp; OSM Sandbox","text":""},{"location":"about/proposal/#osm-sandbox-preamble","title":"OSM Sandbox (Preamble)","text":"<ul> <li>A while ago we made https://github.com/hotosm/osm-sandbox, in an effort to have a \u2018sandboxed\u2019 OSM backend that could be attached to other services like TM (for private data, demo projects, various use cases).</li> <li>Since then, we have decided to collaborate further with the developmentseed and osmus effort to create a deployment API for temporary osmseed instances.</li> <li>This osm-sandbox-dashboard API allows for osmseed instances to be provisioned on demand, by calling an API, and starting the services within a linked Kubernetes cluster.</li> </ul>"},{"location":"about/proposal/#control-plane-setup","title":"Control Plane Setup","text":"<ul> <li>We can either use EKS, or a custom control plane based on EC2 instances.</li> <li>Storage and networking must be configured.</li> <li>We can set up Gateway API or Ingress, plus certificate management of some sort.</li> <li>We need at least one attached worker node to deploy services onto.</li> </ul>"},{"location":"about/proposal/#end-goals","title":"End Goals","text":"<ul> <li>A working Kubernetes cluster that we can deploy services into.</li> <li>A configured Helm chart (already exists) for the osm-sandbox-dashboard.</li> <li>Accessible via a URL - perhaps we have a DNS zone specifically for this, plus CNAME aliases to specific services.</li> <li>Also nice to have: ArgoCD with the config for osm-sandbox-dashboard pulled from a repo, plus an easy visualisation dashboard of running services in the cluster.</li> </ul>"},{"location":"about/proposal/#step-2-deployment-of-easier-hotosm-apps","title":"Step 2: Deployment of Easier HOTOSM Apps","text":"<p>These apps have fewer moving parts, or are easier to package up and deploy (FieldTM has a partial helm chart already).</p> <ul> <li>FieldTM</li> <li>DroneTM</li> <li>Export Tool / Raw-Data-API (in order)</li> </ul>"},{"location":"about/proposal/#fieldtm","title":"FieldTM","text":"<ul> <li>Some of the requirements for FieldTM are already captured in issues here: https://github.com/hotosm/field-tm/issues?q=is%3Aissue%20state%3Aopen%20label%3Adevops</li> <li>FieldTM requires a deployment of ODK alongside it, meaning we also need to make a helm chart for that (it would be great to contribute to the community, but first we should discuss with the ODK team).</li> </ul>"},{"location":"about/proposal/#dronetm","title":"DroneTM","text":"<ul> <li>The deployment of DroneTM will be quite similar to FieldTM, but instead of a requirement for ODK, we also need to deploy OpenDroneMap, with NodeODM being scalable via CPU utilisation or queue length with a tool like KEDA.</li> </ul>"},{"location":"about/proposal/#export-tool-raw-data-api","title":"Export Tool / Raw-Data-API","text":"<ul> <li>Includes a Celery task queue.</li> <li>More notes to come.</li> </ul>"},{"location":"about/proposal/#step-3-deployment-of-more-difficult-hotosm-apps","title":"Step 3: Deployment of More Difficult HOTOSM Apps","text":"<p>These apps have many moving parts that must be replaced from their AWS specific service to a more vendor-neutral alternative.</p> <ul> <li>Tasking Manager</li> <li>fAIr</li> <li>OpenAerialMap (in order)</li> </ul>"},{"location":"tutorial/certs-and-dns/","title":"Automated Certificate &amp; DNS Management","text":""},{"location":"tutorial/deploying-apps/","title":"Deploying Apps In Kubernetes","text":"<p>From skillshare session 22/10/2025</p>"},{"location":"tutorial/deploying-apps/#namespaces","title":"Namespaces","text":"<ul> <li>Namespaces are logical partitions within a Kubernetes cluster.</li> </ul> <ul> <li>They allow you to group related resources and apply policies   (role based authentication RBAC, resource quotas, network policies).</li> </ul> <pre><code>kubectl create namespace oam\n</code></pre> <ul> <li>While it's possible to run a namespace per deployment   environment - dev/stage/prod - it's a bit cleaner to   have a separate cluster per environment.</li> </ul> <ul> <li>Namespaces can be used to easily organise logical   application units, e.g. a <code>oam</code>, or <code>imagery</code> namespace.</li> </ul>"},{"location":"tutorial/deploying-apps/#anatomy-of-manifests","title":"Anatomy of manifests","text":"<ul> <li>Every Kubernetes manifest follows the same high-level structure:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.27-alpine\n</code></pre> Field Purpose apiVersion Defines which API group/version the resource uses. kind The type of resource (<code>Pod</code>, <code>Service</code>, <code>Deployment</code>, etc.). metadata Identifiers: name, labels, annotations, namespace. spec The desired configuration (replicas, template, ports, etc.)."},{"location":"tutorial/deploying-apps/#networking-services-ingress","title":"Networking: Services &amp; Ingress","text":""},{"location":"tutorial/deploying-apps/#services","title":"Services","text":"<ul> <li>A Service defines how to reach a set of Pods inside the cluster.</li> </ul> <ul> <li>Each Service has a stable virtual IP (ClusterIP) and DNS name,   and routes traffic to all matching Pods via labels.</li> </ul> <ul> <li>Service types:<ul> <li>ClusterIP: internal only (default)</li> <li>NodePort: expose a port on each node</li> <li>LoadBalancer: integrate with cloud load balancers</li> <li>ExternalName: DNS alias for external resources</li> </ul> </li> </ul> <p>Example:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: oam\nspec:\n  selector:\n    app: nginx\n  ports:\n    - port: 80\n      targetPort: 80\n  type: ClusterIP\n</code></pre> <ul> <li>Typically ClusterIP will be used for most apps, with   an Ingress defined for the actual external access.</li> </ul>"},{"location":"tutorial/deploying-apps/#ingress","title":"Ingress","text":"<ul> <li>An Ingress defines external access to Services, typically via HTTP/HTTPS.</li> </ul> <ul> <li>It acts as a router or reverse proxy, mapping domain names and paths to Services.</li> </ul> <ul> <li>Requires an Ingress Controller (e.g., NGINX, AWS ALB, Traefik).</li> </ul> <p>Example:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx-ingress\n  namespace: oam\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n    - host: nginx.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: nginx-service\n                port:\n                  number: 80\n</code></pre>"},{"location":"tutorial/deploying-apps/#practical-make-nginx-accessible","title":"Practical: make Nginx accessible","text":"<ul> <li>From previous example, add the Service and Ingress definitions   above to the same <code>nginx.yaml</code>, with each divided by <code>---</code> between.</li> </ul> <pre><code># Apply over the top\nkubectl apply -f nginx.yaml -n oam\n\n# Verify\nkubectl get ingress -n oam\nkubectl get svc -n oam\n</code></pre> <ul> <li>Access on <code>http://nginx.local</code>.</li> </ul> <ul> <li>Alternatively, we can do a port forward to access the internal   service:</li> </ul> <pre><code>kubectl port-forward svc/nginx-service 8080:80\n</code></pre>"},{"location":"tutorial/deploying-apps/#probes","title":"Probes","text":"<ul> <li>Probes let Kubernetes know whether your container is healthy   and ready for traffic.</li> </ul> <ul> <li>Liveness Probe: checks if the container is still running   properly. If it fails repeatedly, Kubernetes restarts the   container.</li> </ul> <ul> <li>Readiness Probe: checks if the app is ready to serve traffic.   If it fails, the pod is temporarily removed from Service endpoints.</li> </ul>"},{"location":"tutorial/deploying-apps/#practical-add-probes-to-nginx","title":"Practical: add probes to Nginx","text":"<ul> <li>Add the following to the deployment spec in   <code>nginx.yaml</code>:</li> </ul> <pre><code>          readinessProbe:\n            httpGet:\n              path: /\n              port: 80\n            initialDelaySeconds: 3\n            periodSeconds: 5\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 80\n            initialDelaySeconds: 10\n            periodSeconds: 10\n</code></pre> <pre><code># Apply over the top\nkubectl apply -f nginx.yaml -n oam\n\n# View pod health\nkubectl get pod -n oam\nkubectl describe pod &lt;pod-name&gt; -n oam\n</code></pre>"},{"location":"tutorial/deploying-apps/#resource-constraints","title":"Resource constraints","text":"<ul> <li>Resource requests and limits prevent a single container   from consuming too many cluster resources.</li> </ul> Type Purpose requests The minimum guaranteed CPU/memory the Pod needs. limits The maximum it can consume."},{"location":"tutorial/deploying-apps/#practical-add-resouce-constraints-to-nginx","title":"Practical: add resouce constraints to Nginx","text":"<ul> <li>Add the following to the deployment spec in   <code>nginx.yaml</code>:</li> </ul> <pre><code>          resources:\n            requests:\n              cpu: 50m\n              memory: 64Mi\n            limits:\n              cpu: 250m\n              memory: 128Mi\n</code></pre> <pre><code># Apply over the top\nkubectl apply -f nginx.yaml -n oam\n\n# View pod health\nkubectl get pod -n oam\nkubectl describe pod &lt;pod-name&gt; -n oam\nkubectl top pods -n oam\n</code></pre> <p>Note</p> <pre><code>1 CPU = 1 vCPU core, 1000m = 1 core.\nKubernetes schedules pods based on **requests**.\n**Limits** enforce hard caps.\n</code></pre>"},{"location":"tutorial/deploying-apps/#rolling-updates-and-rollbacks","title":"Rolling updates and rollbacks","text":"<ul> <li>Deployments automatically perform rolling updates,   replacing pods gradually to avoid downtime.</li> </ul> <pre><code># Upgrade the container version using rolling update\nkubectl set image deployment/nginx-deployment nginx=nginx:1.28-alpine -n oam\n\n# Monitor update\nkubectl rollout status deployment/nginx-deployment -n oam\n\n# Rollback\nkubectl rollout undo deployment/nginx-deployment -n oam\n</code></pre>"},{"location":"tutorial/gitops-argocd/","title":"Deploying Apps With GitOps and ArgoCD","text":""},{"location":"tutorial/k8s-intro/","title":"Kubernetes Introduction","text":"<p>From skillshare session 15/10/2025</p>"},{"location":"tutorial/k8s-intro/#video-recording","title":"Video Recording","text":""},{"location":"tutorial/k8s-intro/#what-is-kubernetes","title":"What is Kubernetes?","text":"<ul> <li>Building on the concept of containers to deploy applications,   Kubernetes (often abbreviated as k8s) is an orchestration tool,   for linking multiple servers together, sharing load, and running   resources across the cluster.</li> </ul> <ul> <li>Kubernetes is delcarative. You specify your 'desired'   state, e.g. I want 2 copies of this app running, and   the Kubernetes control plane will try it's best to keep   everything you specified running. Hence 'self-healing'.</li> </ul> <ul> <li>At it's core, Kubernetes is a collection of tools mostly maintained   by the Cloud Native Computing Foundation(CNCF):<ul> <li>Container runtime: low level runtime such as <code>containerd</code> + <code>runc</code>.</li> <li>Key-value database: storing the state of the cluster, <code>etcd</code>.</li> <li>Network router: route traffic between services, <code>kube-proxy</code>.</li> <li>DNS server: make services discoverable, <code>CoreDNS</code>.</li> <li>Scheduler: assign workloads (Pods) to suitable nodes.</li> <li>Node agent: runs on each node, managing pods / containers, <code>kubelet</code>.</li> <li>Plugins: storage and networking extensions built on standard   Linux tools.</li> </ul> </li> </ul> <p>Note</p> <pre><code>As we are running multiple machines across a network, or\npossibly multiple networks, we need to consider distributed\ncomputing concepts.\n\nRunning via docker is pretty simple, as you only have a single\nmachine. Running via Kubernetes requires some thought to\nnetworking across machines, and shared distributed storage\nmethods.\n</code></pre>"},{"location":"tutorial/k8s-intro/#node-types","title":"Node types","text":"<ul> <li>'Nodes' are simply machines in the cluster.</li> </ul> <ul> <li>There are two types:<ul> <li>Control plane: run the core components needed for the cluster,   manage state, and run commands to keep reality in line with   desired state.</li> <li>Worker: the machines that run actual workloads, such as   applications, background jobs, data pipelines, etc.</li> </ul> </li> </ul> <ul> <li>Note that worker nodes can optionally have GPUs attached, and be   designated 'GPU nodes'.</li> </ul>"},{"location":"tutorial/k8s-intro/#resource-types","title":"Resource types","text":"<ul> <li>There are too many to list here, but the key ones you need are:<ul> <li>Pods: essentially a wrapper around a container (\u00b1 initContainer).</li> <li>Deployment: defines how many copies of a pod should run across nodes.</li> <li>Service: provides a network endpoint and load balancing for pods.</li> <li>Ingress: exposes HTTP/HTTPS applications externally, often using   domain names and TLS certificates.</li> <li>ConfigMap / Secret: define configuration or credentials for apps.</li> <li>Namespace: a separated group of resources, for better organisation /   isolation (for example, group all pods that make up a single app). Also   helps with namespace-level access control when dealing with many   contractors / devs.</li> </ul> </li> </ul> <p>Note</p> <p>Typically you will be using Deployments to manage your applications. Depoyments manage a ReplicaSet underneath, i.e. a specific number of pods, plus the actual pods that are running. ReplicaSets are rarely used on their own, unless specific fine grained control is needed.</p> <p>There are two other type of 'Sets' being the StatefulSet and DaemonSet.</p> <p>DaemonSets: runs one pod per node. Useful for daemon such a log   collectors or monitoring agents. StatefulSet: for stateful apps that need stable, unique pod   identities and persistent storage. We will cover persistent   storage in a future tutorial.</p>"},{"location":"tutorial/k8s-intro/#yaml-manifests","title":"YAML Manifests","text":"<ul> <li>Each resource in Kubernetes can be defined as a YAML.</li> </ul> <ul> <li>An example deployment for two Nginx pod replicas:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 2                     # desired number of Pod replicas\n  selector:\n    matchLabels:\n      app: nginx                  # label used to match Pods\n  template:\n    metadata:\n      labels:\n        app: nginx                # applied to the Pods themselves\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.27-alpine\n          ports:\n            - containerPort: 80   # port exposed by the container\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop: [\"ALL\"]\n            runAsNonRoot: true\n            seccompProfile:\n              type: RuntimeDefault\n</code></pre>"},{"location":"tutorial/k8s-intro/#practical","title":"Practical","text":""},{"location":"tutorial/k8s-intro/#running-kubernetes-locally-with-talosos","title":"Running Kubernetes locally with TalosOS","text":"<ol> <li>Install Docker.</li> <li>Install TalosCTL: <code>curl -sL https://talos.dev/install | sh</code>.</li> <li>Create a local cluster: <code>talosctl cluster create</code>.</li> <li>Wait a minute, then run workloads with <code>kubectl</code>.</li> <li>Destroy the cluster <code>talosctl cluster destroy</code>.</li> </ol>"},{"location":"tutorial/k8s-intro/#running-commands-in-the-cluster","title":"Running commands in the cluster","text":"<p>Either install tools manually, or use my helper image:</p> <pre><code># Set alias, place in ~/.bashrc if you prefer\nalias aws-shell='docker run --rm -it --name aws-cli -v $HOME:/root -v /var/run/docker.sock:/var/run/docker.sock --workdir /root --network host ghcr.io/spwoodcock/awscli-kubectl:latest'\n\naws-shell\n\n# Connect to cluster\nkcc\n\n# View cluster details\nkubectl get node\nkubectl get pods --all-namespaces\n\n# Change namespace\nns\n</code></pre>"},{"location":"tutorial/k8s-intro/#deploy-nginx-test-app","title":"Deploy Nginx test app","text":"<ul> <li>Using the Nginx deployment YAML defined above,   create the file <code>nginx.yaml</code>.</li> </ul> <ul> <li>Next, apply the deployment to the cluster:</li> </ul> <pre><code>kubectl apply -f nginx.yaml\n\nkubectl get pods\n</code></pre> <ul> <li>Let's scale the deployment to 4 replicas:</li> </ul> <pre><code>kubectl scale deployment nginx-deployment --replicas=4\n</code></pre> <ul> <li>View the details of a pod:</li> </ul> <pre><code>kubectl get pods\nkubectl describe pod/nginx-deployment-xxxx-xxxx\nkubectl logs pod/nginx-deployment-xxxx-xxxx\n</code></pre> <ul> <li>Then delete the deployment:</li> </ul> <pre><code>kubectl delete -f nginx.yaml\n</code></pre>"},{"location":"tutorial/secrets-management/","title":"Secrets Management In Kubernetes","text":"<p>From skillshare session 26/11/2025</p>"},{"location":"tutorial/secrets-management/#video-recording","title":"Video Recording","text":""},{"location":"tutorial/secrets-management/#how-do-secrets-work-in-kubernetes","title":"How do secrets work in Kubernetes?","text":"<ul> <li>Kubernetes stores secrets as plain text = compromised cluster = compromised secrets.</li> <li>This is generally fine though: all depends on your threat model.</li> <li>SealedSecrets facilitate a GitOps approach and keep secrets outside   of password managers, and inside Git repos - fully encrypted.</li> </ul>"},{"location":"tutorial/secrets-management/#install-bitnami-sealed-secrets","title":"Install Bitnami Sealed Secrets","text":"<ol> <li>Deploy using ArgoCD:</li> </ol> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: sealed-secrets\n  namespace: argocd\nspec:\n  project: default\n  source:\n    chart: sealed-secrets\n    repoURL: https://bitnami-labs.github.io/sealed-secrets\n    targetRevision: 2.18.0\n    # We override keyrenewperiod this to provide a simpler GitOps experience.\n    #\n    # With key renewal enabled - best practice - this means the master key is\n    # renewed every 30 days.\n    # \n    # In the event of recovery, we need to have made sure we had the latest key\n    # backed up - this is unrealistic.\n    #\n    # Options to backup via cron into object storage are undesirable.\n    # Simply keeping the primary key very safe is a simple solution.\n    # A tradeoff between operational complexity and security.\n    helm:\n      releaseName: sealed-secrets-controller\n      valuesObject:\n        fullnameOverride: sealed-secrets-controller\n        keyrenewperiod: 0\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: kube-system\n  syncPolicy:\n    automated:\n      selfHeal: true\n      prune: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <ol> <li>Install <code>kubeseal</code>:</li> </ol> <ul> <li>Present in <code>ghcr.io/spwoodcock/awscli-kubectl:latest</code></li> <li>Or install manually: https://github.com/bitnami-labs/sealed-secrets/releases</li> </ul> <ol> <li>Backup Sealed Secrets Key:</li> </ol> <pre><code>kubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml &gt;main.key\n</code></pre> <p>Store this securely!</p>"},{"location":"tutorial/secrets-management/#create-sealed-secrets","title":"Create Sealed Secrets","text":""},{"location":"tutorial/secrets-management/#using-from-literal-simple-values","title":"Using --from-literal (simple values)","text":"<pre><code>kubectl create secret generic cloudflare-api \\\n    --from-literal=ACCOUNT_ID=your-account-id \\\n    --from-literal=TOKEN=your-token \\\n    --dry-run=client \\\n    --namespace='gateway' \\\n    -o yaml &gt; secret.yaml\n</code></pre>"},{"location":"tutorial/secrets-management/#using-from-file-file-contents","title":"Using --from-file (file contents)","text":"<pre><code># Single file\nkubectl create secret generic my-secret \\\n    --from-file=config.json \\\n    --dry-run=client -o yaml &gt; secret.yaml\n\n# Multiple files\nkubectl create secret generic tls-secret \\\n    --from-file=tls.crt \\\n    --from-file=tls.key \\\n    --dry-run=client -o yaml &gt; secret.yaml\n\n# Custom key name\nkubectl create secret generic my-secret \\\n    --from-file=my-config=config.json \\\n    --dry-run=client -o yaml &gt; secret.yaml\n</code></pre>"},{"location":"tutorial/secrets-management/#seal-and-commit","title":"Seal and commit","text":"<pre><code>kubeseal -f secret.yaml -w sealed-secret.yaml\nrm secret.yaml\ngit add sealed-secret.yaml\n</code></pre>"},{"location":"tutorial/secrets-management/#update-existing-sealed-secret","title":"Update Existing Sealed Secret","text":"<ol> <li>Get current secret from cluster:</li> </ol> <pre><code>kubectl get secret cloudflare-api -n gateway -o yaml\n</code></pre> <ol> <li>View current values:</li> </ol> <pre><code>kubectl get secret cloudflare-api -n gateway -o jsonpath='{.data.TOKEN}' | base64 -d\n</code></pre> <ol> <li>Recreate with new values:</li> </ol> <pre><code>kubectl create secret generic cloudflare-api \\\n    --from-literal=ACCOUNT_ID=new-account-id \\\n    --from-literal=TOKEN=new-token \\\n    --dry-run=client \\\n    --namespace='gateway' \\\n    -o yaml &gt; secret.yaml\n</code></pre> <ol> <li>Re-seal and commit:</li> </ol> <pre><code>kubeseal -f secret.yaml -w sealed-secret.yaml\nrm secret.yaml\ngit add sealed-secret.yaml\ngit commit -m \"Update sealed secret\"\n</code></pre>"},{"location":"tutorial/secrets-management/#emergency-recovery","title":"Emergency Recovery","text":"<p>To retrieve a secret from a sealed secret file (offline):</p> <ul> <li>Get the saved master key.</li> <li>Then run:</li> </ul> <pre><code>kubeseal \\\n  --recovery-unseal \\\n  --recovery-private-key master-key.key \\\n  -f sealed-secret.yaml -o yaml &gt; secret.yaml\n</code></pre> <ul> <li>This will produce a normal Kubernetes secret, which you can base64   decode as needed.</li> </ul>"},{"location":"tutorial/secrets-management/#other-options","title":"Other Options","text":"<ul> <li>Cloud provider integrations (AWS Secrets Manager, etc.) - not portable</li> <li>External Secrets Operator with Hashicorp Vault - requires external service</li> <li>SealedSecrets = simple, portable, GitOps-friendly</li> </ul>"},{"location":"tutorial/storage/","title":"Kubernetes Storage","text":"<p>Kubernetes doesn't provide storage - you need an external system.</p>"},{"location":"tutorial/storage/#storage-classes","title":"Storage Classes","text":"<p>Storage Classes are Kubernetes' abstraction layer for dynamic provisioning. They define: - Which provisioner to use (e.g., AWS EBS, Longhorn, NFS) - Performance characteristics (SSD vs HDD, replication factor) - Reclaim policies (delete or retain volumes when claims are deleted)</p> <p>When you create a PersistentVolumeClaim (PVC), Kubernetes automatically provisions storage from the specified StorageClass.</p> <p>Most clusters have a default StorageClass, so you often don't need to specify one explicitly.</p>"},{"location":"tutorial/storage/#options","title":"Options","text":""},{"location":"tutorial/storage/#localhostpath","title":"Local/HostPath","text":"<ul> <li>Data stored directly on node disk.</li> <li>Fast but not portable, no redundancy.</li> <li>Use only for caching, logs, ephemeral data.</li> </ul>"},{"location":"tutorial/storage/#longhorn","title":"Longhorn","text":"<ul> <li>Simple replicated storage across nodes.</li> <li>Lightweight, but not very performant in prod.</li> <li>Easy setup, good for homelabs and small clusters.</li> </ul>"},{"location":"tutorial/storage/#cloud-volumes-aws-ebs-gcp-pd-azure-disk","title":"Cloud Volumes (AWS EBS, GCP PD, Azure Disk)","text":"<ul> <li>Fully managed by cloud provider.</li> <li>Extremely reliable, zero ops overhead.</li> <li>Trade-off: vendor lock-in and cost.</li> </ul>"},{"location":"tutorial/storage/#ceph-rook","title":"Ceph / Rook","text":"<ul> <li>Powerful distributed storage with full control.</li> <li>Complex to operate - needs a lot of compute available.</li> <li>Rock-solid, but only for large on-prem deployments with expertise.</li> </ul>"},{"location":"tutorial/storage/#nfs","title":"NFS","text":"<ul> <li>Simple shared storage.</li> <li>Easy setup, universal compatibility.</li> <li>Performance depends on NFS server.</li> <li>Fine for shared files, avoid for databases.</li> <li>Probably also only good for a homelab mostly.</li> </ul>"},{"location":"tutorial/storage/#quick-guide","title":"Quick Guide","text":"<ul> <li>Homelab --&gt; Longhorn</li> <li>Production --&gt; Cloud volumes</li> <li>Large on-prem --&gt; Ceph (if you have the expertise)</li> <li>Testing/cache --&gt; Local volumes</li> <li>Shared files --&gt; NFS</li> </ul>"},{"location":"tutorial/troubleshooting/","title":"Troubleshooting","text":""},{"location":"tutorial/troubleshooting/#resource-stuck-in-terminating-state","title":"Resource stuck in 'terminating' state","text":"<p>If it's been stuck for a long time (hours - days), then modifiy the finalizer to allow the resource to terminate:</p> <p>namespace <pre><code>kubectl get namespace \"stuck-namespace\" -o json \\\n  | tr -d \"\\n\" | sed \"s/\\\"finalizers\\\": \\[[^]]\\+\\]/\\\"finalizers\\\": []/\" \\\n  | kubectl replace --raw /api/v1/namespaces/stuck-namespace/finalize -f -\n</code></pre></p> <p>pod <pre><code>kubectl get pod stuck-pod-name -n drone -o json \\\n| jq 'del(.metadata.finalizers)' \\\n| kubectl replace --raw \"/api/v1/namespaces/drone/pods/stuck-pod-name/finalize\" -f -\n</code></pre></p>"},{"location":"usage/connecting-to-cluster/","title":"Connecting To The Cluster","text":"<ul> <li>You will need both <code>aws-cli</code> and <code>kubectl</code> installed.</li> </ul>"},{"location":"usage/connecting-to-cluster/#optional-run-inside-a-container","title":"Optional: Run Inside A Container","text":"<ul> <li>To avoid installing heavy dependencies, accessing the cluster can be    done via container.</li> <li>Pull this image:   <code>ghcr.io/spwoodcock/awscli-kubectl</code></li> <li>Then create an alias:   <code>alias aws-shell='docker run --rm -it --network=host -v $HOME:/root --workdir /root ghcr.io/spwoodcock/awscli-kubectl:latest'</code></li> <li>Simply run <code>aws-shell</code> before continuing below.</li> </ul>"},{"location":"usage/connecting-to-cluster/#configure-aws-cli","title":"Configure AWS CLI","text":"<pre><code># Configure SSO\naws configure sso --use-device-code\n\n# Enter details\n# Session name: admin\n# Start URL: https://hotosm.awsapps.com/start/#\n# Start region: eu-west-1\n# Then login, and set profile name = admin\n\n# Login to SSO\naws sso login --profile admin --use-device-code\n\n# Gen kubeconfig (automatically appends to existing kubeconfig)\naws eks update-kubeconfig --profile admin --name hotosm-production-cluster --region us-east-1\n</code></pre>"},{"location":"usage/connecting-to-cluster/#use-kubectl","title":"Use Kubectl","text":"<pre><code># If you are still logged in, ignore this step, otherwise\naws sso login --profile admin --use-device-code\n\nkubectl get pods\n</code></pre>"},{"location":"usage/connecting-to-cluster/#read-only-cluster-role","title":"Read-Only Cluster Role","text":"<p>For contractors to view status of deployments etc.</p>"},{"location":"usage/connecting-to-cluster/#creating-it-as-admin","title":"Creating it as admin","text":"<ul> <li>Got to IAM Identity Center in the correct region.</li> <li>Add users + create a group for the users.</li> <li>Create a Permission Set with <code>AmazonEKSMCPReadOnlyAccess</code>.</li> <li>Create an 'AWS Account' that links the role and Permission Set.</li> <li>This is used as the <code>arn</code> below:</li> </ul> <pre><code># Get ARN\naws iam get-role \\\n    --profile admin \\\n    --role-name AWSReservedSSO_ReadOnlyClusterAccessPermission_b0f9a40b216948f7\n\n# Create access entry\naws eks create-access-entry \\\n  --profile admin \\\n  --cluster-name hotosm-production-cluster \\\n  --principal-arn 'arn:aws:iam::670261699094:role/aws-reserved/sso.amazonaws.com/eu-west-1/AWSReservedSSO_ReadOnlyClusterAccessPermission_b0f9a40b216948f7' \\\n  --type STANDARD \\\n  --region us-east-1\n\n# Associate access policy\naws eks associate-access-policy \\\n  --profile admin \\\n  --cluster-name hotosm-production-cluster \\\n  --principal-arn 'arn:aws:iam::670261699094:role/aws-reserved/sso.amazonaws.com/eu-west-1/AWSReservedSSO_ReadOnlyClusterAccessPermission_b0f9a40b216948f7' \\\n  --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSViewPolicy \\\n  --access-scope type=cluster \\\n  --region us-east-1\n\n# Check - if eksctl is available on your system\neksctl get iamidentitymapping --cluster hotosm-production-cluster \\\n  --region us-east-1 \\\n  --profile readonly\n</code></pre>"},{"location":"usage/connecting-to-cluster/#using-the-role","title":"Using the role","text":"<p>This role should have access to view pods / deployment progress, but not modify things / exec / view secrets.</p> <p><code>~/.aws/config</code> <pre><code>[profile readonly]\nsso_session = readonly\nsso_account_id = 670261699094\nsso_role_name = ReadOnlyClusterAccessPermission\n[sso-session readonly]\nsso_start_url = https://hotosm.awsapps.com/start/#\nsso_region = eu-west-1\nsso_registration_scopes = sso:account:access\n</code></pre></p> <p>Terminal: <pre><code># Login to SSO\naws sso login --profile readonly --use-device-code\n\n# Update Kubeconfig with access\naws eks update-kubeconfig --profile readonly --name hotosm-production-cluster --region us-east-1\n\n# View pods\nkubectl get pods\n</code></pre></p>"},{"location":"usage/routing-to-cluster-apps/","title":"Routing To Cluster Applications","text":"<p>The networking chain is like this:</p> <p>User --&gt; AWS Route53 DNS Record --&gt; AWS Elastic Load Balancer (DualStack) --&gt; EKS Cluster Endpoint --&gt; k8s Ingress --&gt; k8s Service --&gt; k8s Application</p>"},{"location":"usage/routing-to-cluster-apps/#provisioning-dns-records","title":"Provisioning DNS Records","text":""},{"location":"usage/routing-to-cluster-apps/#automatic-dns-external-dns","title":"Automatic DNS (external-dns)","text":"<ul> <li>We have the <code>external-dns</code> operator installed in the cluster, which   can automatically provision Route53 domains on request.</li> <li>Authentication with Route53 is done via static credentials:<ul> <li>This means we simply use an IAM User with assigned access/secret   key pair.</li> <li>Typically IAM Roles for Service Accounts are recommended instead,   for generation or automatic temporary credentials.</li> <li>We use static credentials for (1) simplicity (2) less reliance   on AWS specific config, allowing for easier migration away if   needed.</li> <li>To improve security, we strictly limit IAM access for the user.</li> </ul> </li> </ul> <p>There is a great guide for doing this in the official external-dns docs, but the main steps are below.</p>"},{"location":"usage/routing-to-cluster-apps/#configuring-external-dns","title":"Configuring external-dns","text":"<ul> <li>Create policy.json</li> </ul> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:route53:::hostedzone/Z007708822IRWZYIAATO8\",\n        \"arn:aws:route53:::hostedzone/Z01079471L60VTHK00IFQ\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\",\n        \"route53:ListTagsForResource\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n</code></pre> <ul> <li>Create a policy from <code>policy.json</code>:</li> </ul> <pre><code>aws --profile admin iam create-policy --policy-name \"k8sExternalDnsRoute53\" --policy-document file://policy.json\n\n# example: arn:aws:iam::XXXXXXXXXXXX:policy/k8sExternalDnsRoute53\nexport POLICY_ARN=$(aws --profile admin iam list-policies \\\n --query 'Policies[?PolicyName==`k8sExternalDnsRoute53`].Arn' --output text)\n</code></pre> <ul> <li>Create users with the attached policy:</li> </ul> <pre><code># create IAM user\naws --profile admin iam create-user --user-name \"externaldns\"\n\n# attach policy arn created earlier to IAM user\naws --profile admin iam attach-user-policy --user-name \"externaldns\" --policy-arn $POLICY_ARN\n</code></pre> <ul> <li>Create security creds:</li> </ul> <pre><code>SECRET_ACCESS_KEY=$(aws --profile admin iam create-access-key --user-name \"externaldns\")\nACCESS_KEY_ID=$(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.AccessKeyId')\n\ncat &lt;&lt;-EOF &gt; credentials\n[default]\naws_access_key_id = $(echo $ACCESS_KEY_ID)\naws_secret_access_key = $(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.SecretAccessKey')\nEOF\n</code></pre> <ul> <li>Create a sealed secret for storing in this repo:</li> </ul> <pre><code>kubectl create secret generic external-dns-aws-creds \\\n  --from-file=credentials=./credentials \\\n  --namespace kube-system \\\n  --dry-run=client -o yaml &gt; secret.yaml\n\nkubeseal -f secret.yaml -w sealed-secret.yaml\n# Move the file to apps/external-dns/sealed-secret.yaml\n# The secret.yaml can be discarded\n</code></pre>"},{"location":"usage/routing-to-cluster-apps/#using-external-dns","title":"Using external-dns","text":"<ul> <li>It' works by adding annotations such as:   <code>external-dns.alpha.kubernetes.io/hostname: api.imagery.hotosm.org</code></li> <li>The annotation is picked up when an Ingress is made, and the   DNS entry is automatically made in Route53.</li> </ul>"},{"location":"usage/routing-to-cluster-apps/#updating-external-dns","title":"Updating external-dns","text":"<ul> <li>To update in future (e.g. update version), modify the file:   <code>apps/external-dns.yaml</code></li> <li>To add a new hosted zone:<ol> <li>Modify the IAM Policy to include the Zone ID.</li> <li>Modify the helm values to include the domain.</li> </ol> </li> </ul>"},{"location":"usage/routing-to-cluster-apps/#manual-dns","title":"Manual DNS","text":"<ul> <li>This shouldn't need to be done, but just for information   about how this works.</li> <li>Go to Route53 and create a new A record.</li> <li>Check 'Alias' and 'Alias to Application and Classic Load Balancer'.</li> <li>Set region to <code>us-east-1</code>.</li> <li>We are currently using 'classic' load balancer:   <code>dualstack.a1294ee7c4d2e41de88a0a5451b065b4-1435866857.us-east-1.elb.amazonaws.com.</code></li> </ul>"}]}