{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HOT\u2019s Infrastructure Modernization: Kubernetes","text":"<p>[!Note] Currently under initial development. </p> <p>Kubernetes @ Humanitarian OpenStreetMap Team (HOT).</p> <p>See the inital proposal for more background.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#required-tools","title":"Required Tools","text":"<ul> <li>AWS CLI</li> <li>OpenTofu</li> <li>kubectl</li> <li>Helm</li> </ul>"},{"location":"#areas-for-further-initial-development","title":"Areas for Further (Initial) Development","text":""},{"location":"#variable-management","title":"Variable Management","text":"<ul> <li>Duplication exists between TF inputs, CI workflows, and local scripts.</li> <li>A tool like https://github.com/helmfile/helmfile may help with sourcing variables by environment.     - A basic version has been added to deploy revision deltas, further templating would be required.</li> <li>As more HOT applications + services are moved to cluster, this will only grow.</li> </ul>"},{"location":"#deployment","title":"Deployment","text":"<ul> <li>Provisioning is currently done in the same workflow (TF, K8s, Helm), mostly as byproduct of initial development phase. Can be further refined.</li> <li>GitOps tools like ArgoCD are in the works</li> <li>Flux Tofu controller may be an analog for base infrastructure (further investigation required).</li> </ul>"},{"location":"#bridging-tf-and-kubernetes","title":"Bridging TF and Kubernetes","text":"<ul> <li>TF-managed information often needs to be referenced on the cluster     - ex: PostgresCluster CRD requires the role ARN authorized for backups. Role and bucket are created in TF.</li> <li>Global cluster resources are provisioned through TF, but argument can be made for their management by K8s. </li> <li>Ideal solution enables cluster resources to reference, mount, inject, etc. TF-managed information with minimal developer intervention.</li> </ul>"},{"location":"about/proposal/","title":"HOT\u2019s Infrastructure Modernization: Kubernetes","text":""},{"location":"about/proposal/#requirement","title":"Requirement","text":"<ul> <li>HOT is quite reliant on vendor-specific cloud services for various tools.</li> <li>We rely heavily on donated credits from cloud providers (today, we fully rely on AWS) and have minimal internal funding for infrastructure. This source comes with few guarantees! </li> <li>Ideally we would have a cloud-agnostic approach to deployment, giving the flexibility to deploy anywhere if the case arises (AWS, Azure, Hetzner, on-prem).</li> <li>This is not a small task, and also won\u2019t solve all of our problems, but it would be a great start to migrate as many services as possible to cloud-agnostic approaches, via open tools such as Kubernetes, KEDA (scaling), ArgoCD (GitOps), etc.</li> </ul>"},{"location":"about/proposal/#challenges","title":"Challenges","text":"<ul> <li>Lack of time and resources to dedicate to this as a project - it\u2019s difficult to justify addressing tech debt, when there is the allure of new features and updating software.</li> <li>Lack of expertise in the tech team <ul> <li>Tech lead has previous k8s experience, but little time. </li> <li>Some devs at our partner NAXA have also dabbled with k8s.</li> </ul> </li> <li>Heavy reliance of some tools on vendor-specific services.</li> </ul> <p>In order of most difficult \u2192 least difficult for migration in our 2024 assessment:   - OpenAerialMap (many AWS services / lambda etc).   - fAIr (GPU reliant ML workflows, task scheduling, autoscaling).   - Tasking Manager (autoscaling requirement, large user base).   - Export tool / raw-data-api (task scheduling, redis queue based autoscaling)   - FieldTM &amp; DroneTM</p>"},{"location":"about/proposal/#benefits","title":"Benefits","text":"<ul> <li>We would like to slowly start to become more cloud-agnostic in our deployment approach, making us more resilient to changes in the future.</li> <li>Reduced costs \ud83e\udd1e, with resource utilization spread across a smaller cluster of VMs, instead of many under-utilized standalone VMs, but still able to handle load spikes.</li> </ul>"},{"location":"about/proposal/#proposal","title":"Proposal","text":"<ul> <li>The core of this requirement is to configure a Kubernetes cluster, and start to migrate services into it.</li> <li>This will involve two steps:<ul> <li>Setup of the Kubernetes control plane. Likely EKS, but also open to managing this ourselves with an OS like Talos.</li> <li>Slow migration of services into the cluster, packaging them tools up as Helm charts, and deploying with all additional required components (autoscaling, job queue, etc).</li> </ul> </li> </ul>"},{"location":"about/proposal/#step-1-control-plane-osm-sandbox","title":"Step 1: Control Plane &amp; OSM Sandbox","text":""},{"location":"about/proposal/#osm-sandbox-preamble","title":"OSM Sandbox (Preamble)","text":"<ul> <li>A while ago we made https://github.com/hotosm/osm-sandbox, in an effort to have a \u2018sandboxed\u2019 OSM backend that could be attached to other services like TM (for private data, demo projects, various use cases).</li> <li>Since then, we have decided to collaborate further with the developmentseed and osmus effort to create a deployment API for temporary osmseed instances.</li> <li>This osm-sandbox-dashboard API allows for osmseed instances to be provisioned on demand, by calling an API, and starting the services within a linked Kubernetes cluster.</li> </ul>"},{"location":"about/proposal/#control-plane-setup","title":"Control Plane Setup","text":"<ul> <li>We can either use EKS, or a custom control plane based on EC2 instances.</li> <li>Storage and networking must be configured.</li> <li>We can set up Gateway API or Ingress, plus certificate management of some sort.</li> <li>We need at least one attached worker node to deploy services onto.</li> </ul>"},{"location":"about/proposal/#end-goals","title":"End Goals","text":"<ul> <li>A working Kubernetes cluster that we can deploy services into.</li> <li>A configured Helm chart (already exists) for the osm-sandbox-dashboard.</li> <li>Accessible via a URL - perhaps we have a DNS zone specifically for this, plus CNAME aliases to specific services.</li> <li>Also nice to have: ArgoCD with the config for osm-sandbox-dashboard pulled from a repo, plus an easy visualisation dashboard of running services in the cluster.</li> </ul>"},{"location":"about/proposal/#step-2-deployment-of-easier-hotosm-apps","title":"Step 2: Deployment of Easier HOTOSM Apps","text":"<p>These apps have fewer moving parts, or are easier to package up and deploy (FieldTM has a partial helm chart already).</p> <ul> <li>FieldTM</li> <li>DroneTM</li> <li>Export Tool / Raw-Data-API (in order)</li> </ul>"},{"location":"about/proposal/#fieldtm","title":"FieldTM","text":"<ul> <li>Some of the requirements for FieldTM are already captured in issues here: https://github.com/hotosm/field-tm/issues?q=is%3Aissue%20state%3Aopen%20label%3Adevops</li> <li>FieldTM requires a deployment of ODK alongside it, meaning we also need to make a helm chart for that (it would be great to contribute to the community, but first we should discuss with the ODK team).</li> </ul>"},{"location":"about/proposal/#dronetm","title":"DroneTM","text":"<ul> <li>The deployment of DroneTM will be quite similar to FieldTM, but instead of a requirement for ODK, we also need to deploy OpenDroneMap, with NodeODM being scalable via CPU utilisation or queue length with a tool like KEDA.</li> </ul>"},{"location":"about/proposal/#export-tool-raw-data-api","title":"Export Tool / Raw-Data-API","text":"<ul> <li>Includes a Celery task queue.</li> <li>More notes to come.</li> </ul>"},{"location":"about/proposal/#step-3-deployment-of-more-difficult-hotosm-apps","title":"Step 3: Deployment of More Difficult HOTOSM Apps","text":"<p>These apps have many moving parts that must be replaced from their AWS specific service to a more vendor-neutral alternative.</p> <ul> <li>Tasking Manager</li> <li>fAIr</li> <li>OpenAerialMap (in order)</li> </ul>"},{"location":"tutorial/certs-and-dns/","title":"Automated Certificate &amp; DNS Management","text":""},{"location":"tutorial/deploying-apps/","title":"Deploying Apps In Kubernetes","text":"<p>From skillshare session 22/10/2025</p>"},{"location":"tutorial/deploying-apps/#namespaces","title":"Namespaces","text":"<ul> <li>Namespaces are logical partitions within a Kubernetes cluster.</li> </ul> <ul> <li>They allow you to group related resources and apply policies   (role based authentication RBAC, resource quotas, network policies).</li> </ul> <pre><code>kubectl create namespace oam\n</code></pre> <ul> <li>While it's possible to run a namespace per deployment   environment - dev/stage/prod - it's a bit cleaner to   have a separate cluster per environment.</li> </ul> <ul> <li>Namespaces can be used to easily organise logical   application units, e.g. a <code>oam</code>, or <code>imagery</code> namespace.</li> </ul>"},{"location":"tutorial/deploying-apps/#anatomy-of-manifests","title":"Anatomy of manifests","text":"<ul> <li>Every Kubernetes manifest follows the same high-level structure:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.27-alpine\n</code></pre> Field Purpose apiVersion Defines which API group/version the resource uses. kind The type of resource (<code>Pod</code>, <code>Service</code>, <code>Deployment</code>, etc.). metadata Identifiers: name, labels, annotations, namespace. spec The desired configuration (replicas, template, ports, etc.)."},{"location":"tutorial/deploying-apps/#networking-services-ingress","title":"Networking: Services &amp; Ingress","text":""},{"location":"tutorial/deploying-apps/#services","title":"Services","text":"<ul> <li>A Service defines how to reach a set of Pods inside the cluster.</li> </ul> <ul> <li>Each Service has a stable virtual IP (ClusterIP) and DNS name,   and routes traffic to all matching Pods via labels.</li> </ul> <ul> <li>Service types:<ul> <li>ClusterIP: internal only (default)</li> <li>NodePort: expose a port on each node</li> <li>LoadBalancer: integrate with cloud load balancers</li> <li>ExternalName: DNS alias for external resources</li> </ul> </li> </ul> <p>Example:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: oam\nspec:\n  selector:\n    app: nginx\n  ports:\n    - port: 80\n      targetPort: 80\n  type: ClusterIP\n</code></pre> <ul> <li>Typically ClusterIP will be used for most apps, with   an Ingress defined for the actual external access.</li> </ul>"},{"location":"tutorial/deploying-apps/#ingress","title":"Ingress","text":"<ul> <li>An Ingress defines external access to Services, typically via HTTP/HTTPS.</li> </ul> <ul> <li>It acts as a router or reverse proxy, mapping domain names and paths to Services.</li> </ul> <ul> <li>Requires an Ingress Controller (e.g., NGINX, AWS ALB, Traefik).</li> </ul> <p>Example:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx-ingress\n  namespace: oam\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n    - host: nginx.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: nginx-service\n                port:\n                  number: 80\n</code></pre>"},{"location":"tutorial/deploying-apps/#practical-make-nginx-accessible","title":"Practical: make Nginx accessible","text":"<ul> <li>From previous example, add the Service and Ingress definitions   above to the same <code>nginx.yaml</code>, with each divided by <code>---</code> between.</li> </ul> <pre><code># Apply over the top\nkubectl apply -f nginx.yaml -n oam\n\n# Verify\nkubectl get ingress -n oam\nkubectl get svc -n oam\n</code></pre> <ul> <li>Access on <code>http://nginx.local</code>.</li> </ul> <ul> <li>Alternatively, we can do a port forward to access the internal   service:</li> </ul> <pre><code>kubectl port-forward svc/nginx-service 8080:80\n</code></pre>"},{"location":"tutorial/deploying-apps/#probes","title":"Probes","text":"<ul> <li>Probes let Kubernetes know whether your container is healthy   and ready for traffic.</li> </ul> <ul> <li>Liveness Probe: checks if the container is still running   properly. If it fails repeatedly, Kubernetes restarts the   container.</li> </ul> <ul> <li>Readiness Probe: checks if the app is ready to serve traffic.   If it fails, the pod is temporarily removed from Service endpoints.</li> </ul>"},{"location":"tutorial/deploying-apps/#practical-add-probes-to-nginx","title":"Practical: add probes to Nginx","text":"<ul> <li>Add the following to the deployment spec in   <code>nginx.yaml</code>:</li> </ul> <pre><code>          readinessProbe:\n            httpGet:\n              path: /\n              port: 80\n            initialDelaySeconds: 3\n            periodSeconds: 5\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 80\n            initialDelaySeconds: 10\n            periodSeconds: 10\n</code></pre> <pre><code># Apply over the top\nkubectl apply -f nginx.yaml -n oam\n\n# View pod health\nkubectl get pod -n oam\nkubectl describe pod &lt;pod-name&gt; -n oam\n</code></pre>"},{"location":"tutorial/deploying-apps/#resource-constraints","title":"Resource constraints","text":"<ul> <li>Resource requests and limits prevent a single container   from consuming too many cluster resources.</li> </ul> Type Purpose requests The minimum guaranteed CPU/memory the Pod needs. limits The maximum it can consume."},{"location":"tutorial/deploying-apps/#practical-add-resouce-constraints-to-nginx","title":"Practical: add resouce constraints to Nginx","text":"<ul> <li>Add the following to the deployment spec in   <code>nginx.yaml</code>:</li> </ul> <pre><code>          resources:\n            requests:\n              cpu: 50m\n              memory: 64Mi\n            limits:\n              cpu: 250m\n              memory: 128Mi\n</code></pre> <pre><code># Apply over the top\nkubectl apply -f nginx.yaml -n oam\n\n# View pod health\nkubectl get pod -n oam\nkubectl describe pod &lt;pod-name&gt; -n oam\nkubectl top pods -n oam\n</code></pre> <p>Note</p> <pre><code>1 CPU = 1 vCPU core, 1000m = 1 core.\nKubernetes schedules pods based on **requests**.\n**Limits** enforce hard caps.\n</code></pre>"},{"location":"tutorial/deploying-apps/#rolling-updates-and-rollbacks","title":"Rolling updates and rollbacks","text":"<ul> <li>Deployments automatically perform rolling updates,   replacing pods gradually to avoid downtime.</li> </ul> <pre><code># Upgrade the container version using rolling update\nkubectl set image deployment/nginx-deployment nginx=nginx:1.28-alpine -n oam\n\n# Monitor update\nkubectl rollout status deployment/nginx-deployment -n oam\n\n# Rollback\nkubectl rollout undo deployment/nginx-deployment -n oam\n</code></pre>"},{"location":"tutorial/gitops-argocd/","title":"Deploying Apps With GitOps and ArgoCD","text":""},{"location":"tutorial/k8s-intro/","title":"Kubernetes Introduction","text":"<p>From skillshare session 15/10/2025</p>"},{"location":"tutorial/k8s-intro/#video-recording","title":"Video Recording","text":""},{"location":"tutorial/k8s-intro/#what-is-kubernetes","title":"What is Kubernetes?","text":"<ul> <li>Building on the concept of containers to deploy applications,   Kubernetes (often abbreviated as k8s) is an orchestration tool,   for linking multiple servers together, sharing load, and running   resources across the cluster.</li> </ul> <ul> <li>Kubernetes is delcarative. You specify your 'desired'   state, e.g. I want 2 copies of this app running, and   the Kubernetes control plane will try it's best to keep   everything you specified running. Hence 'self-healing'.</li> </ul> <ul> <li>At it's core, Kubernetes is a collection of tools mostly maintained   by the Cloud Native Computing Foundation(CNCF):<ul> <li>Container runtime: low level runtime such as <code>containerd</code> + <code>runc</code>.</li> <li>Key-value database: storing the state of the cluster, <code>etcd</code>.</li> <li>Network router: route traffic between services, <code>kube-proxy</code>.</li> <li>DNS server: make services discoverable, <code>CoreDNS</code>.</li> <li>Scheduler: assign workloads (Pods) to suitable nodes.</li> <li>Node agent: runs on each node, managing pods / containers, <code>kubelet</code>.</li> <li>Plugins: storage and networking extensions built on standard   Linux tools.</li> </ul> </li> </ul> <p>Note</p> <pre><code>As we are running multiple machines across a network, or\npossibly multiple networks, we need to consider distributed\ncomputing concepts.\n\nRunning via docker is pretty simple, as you only have a single\nmachine. Running via Kubernetes requires some thought to\nnetworking across machines, and shared distributed storage\nmethods.\n</code></pre>"},{"location":"tutorial/k8s-intro/#node-types","title":"Node types","text":"<ul> <li>'Nodes' are simply machines in the cluster.</li> </ul> <ul> <li>There are two types:<ul> <li>Control plane: run the core components needed for the cluster,   manage state, and run commands to keep reality in line with   desired state.</li> <li>Worker: the machines that run actual workloads, such as   applications, background jobs, data pipelines, etc.</li> </ul> </li> </ul> <ul> <li>Note that worker nodes can optionally have GPUs attached, and be   designated 'GPU nodes'.</li> </ul>"},{"location":"tutorial/k8s-intro/#resource-types","title":"Resource types","text":"<ul> <li>There are too many to list here, but the key ones you need are:<ul> <li>Pods: essentially a wrapper around a container (\u00b1 initContainer).</li> <li>Deployment: defines how many copies of a pod should run across nodes.</li> <li>Service: provides a network endpoint and load balancing for pods.</li> <li>Ingress: exposes HTTP/HTTPS applications externally, often using   domain names and TLS certificates.</li> <li>ConfigMap / Secret: define configuration or credentials for apps.</li> <li>Namespace: a separated group of resources, for better organisation /   isolation (for example, group all pods that make up a single app). Also   helps with namespace-level access control when dealing with many   contractors / devs.</li> </ul> </li> </ul> <p>Note</p> <p>Typically you will be using Deployments to manage your applications. Depoyments manage a ReplicaSet underneath, i.e. a specific number of pods, plus the actual pods that are running. ReplicaSets are rarely used on their own, unless specific fine grained control is needed.</p> <p>There are two other type of 'Sets' being the StatefulSet and DaemonSet.</p> <p>DaemonSets: runs one pod per node. Useful for daemon such a log   collectors or monitoring agents. StatefulSet: for stateful apps that need stable, unique pod   identities and persistent storage. We will cover persistent   storage in a future tutorial.</p>"},{"location":"tutorial/k8s-intro/#yaml-manifests","title":"YAML Manifests","text":"<ul> <li>Each resource in Kubernetes can be defined as a YAML.</li> </ul> <ul> <li>An example deployment for two Nginx pod replicas:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 2                     # desired number of Pod replicas\n  selector:\n    matchLabels:\n      app: nginx                  # label used to match Pods\n  template:\n    metadata:\n      labels:\n        app: nginx                # applied to the Pods themselves\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.27-alpine\n          ports:\n            - containerPort: 80   # port exposed by the container\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop: [\"ALL\"]\n            runAsNonRoot: true\n            seccompProfile:\n              type: RuntimeDefault\n</code></pre>"},{"location":"tutorial/k8s-intro/#practical","title":"Practical","text":""},{"location":"tutorial/k8s-intro/#running-kubernetes-locally-with-talosos","title":"Running Kubernetes locally with TalosOS","text":"<ol> <li>Install Docker.</li> <li>Install TalosCTL: <code>curl -sL https://talos.dev/install | sh</code>.</li> <li>Create a local cluster: <code>talosctl cluster create</code>.</li> <li>Wait a minute, then run workloads with <code>kubectl</code>.</li> <li>Destroy the cluster <code>talosctl cluster destroy</code>.</li> </ol>"},{"location":"tutorial/k8s-intro/#running-commands-in-the-cluster","title":"Running commands in the cluster","text":"<p>Either install tools manually, or use my helper image:</p> <pre><code># Set alias, place in ~/.bashrc if you prefer\nalias aws-shell='docker run --rm -it --name aws-cli -v $HOME:/root -v /var/run/docker.sock:/var/run/docker.sock --workdir /root --network host ghcr.io/spwoodcock/awscli-kubectl:latest'\n\naws-shell\n\n# Connect to cluster\nkcc\n\n# View cluster details\nkubectl get node\nkubectl get pods --all-namespaces\n\n# Change namespace\nns\n</code></pre>"},{"location":"tutorial/k8s-intro/#deploy-nginx-test-app","title":"Deploy Nginx test app","text":"<ul> <li>Using the Nginx deployment YAML defined above,   create the file <code>nginx.yaml</code>.</li> </ul> <ul> <li>Next, apply the deployment to the cluster:</li> </ul> <pre><code>kubectl apply -f nginx.yaml\n\nkubectl get pods\n</code></pre> <ul> <li>Let's scale the deployment to 4 replicas:</li> </ul> <pre><code>kubectl scale deployment nginx-deployment --replicas=4\n</code></pre> <ul> <li>View the details of a pod:</li> </ul> <pre><code>kubectl get pods\nkubectl describe pod/nginx-deployment-xxxx-xxxx\nkubectl logs pod/nginx-deployment-xxxx-xxxx\n</code></pre> <ul> <li>Then delete the deployment:</li> </ul> <pre><code>kubectl delete -f nginx.yaml\n</code></pre>"},{"location":"tutorial/secrets-management/","title":"Secrets Management In Kubernetes","text":"<ul> <li>Kubernetes stores secrets as plain text = compromised cluster = compromised secrets.</li> <li>This is generally fine though: all depends on your threat model.</li> <li>SealedSecrets facilitate a GitOps approach and keep secrets outside   of password managers, and inside Git repos - fully encrypted.</li> </ul>"},{"location":"usage/connecting-to-cluster/","title":"Connecting To The Cluster","text":"<ul> <li>You will need both <code>aws-cli</code> and <code>kubectl</code> installed.</li> </ul>"},{"location":"usage/connecting-to-cluster/#optional-run-inside-a-container","title":"Optional: Run Inside A Container","text":"<ul> <li>To avoid installing heavy dependencies, accessing the cluster can be    done via container.</li> <li>Pull this image:   <code>ghcr.io/spwoodcock/awscli-kubectl</code></li> <li>Then create an alias:   <code>alias aws-shell='docker run --rm -it --network=host -v $HOME:/root --workdir /root ghcr.io/spwoodcock/awscli-kubectl:latest'</code></li> <li>Simply run <code>aws-shell</code> before continuing below.</li> </ul>"},{"location":"usage/connecting-to-cluster/#configure-aws-cli","title":"Configure AWS CLI","text":"<pre><code># Configure SSO\naws configure sso --use-device-code\n    Session name: admin\n    Start URL: https://hotosm.awsapps.com/start/#\n    Start region: eu-west-1\n    Then login, and set profile name = admin\n\n# Login to SSO\naws sso login --profile admin --use-device-code\n\n# Gen kubeconfig (automatically appends to existing kubeconfig)\naws eks update-kubeconfig --profile admin --name hotosm-production-cluster --region us-east-1\n</code></pre>"},{"location":"usage/connecting-to-cluster/#use-kubectl","title":"Use Kubectl","text":"<pre><code># If you are still logged in, ignore this step, otherwise\naws sso login --profile admin --use-device-code\n\nkubectl get pods\n</code></pre>"},{"location":"usage/routing-to-cluster-apps/","title":"Routing To Cluster Applications","text":"<p>The networking chain is like this:</p> <p>User --&gt; AWS Route53 DNS Record --&gt; AWS Elastic Load Balancer (DualStack) --&gt; EKS Cluster Endpoint --&gt; k8s Ingress --&gt; k8s Service --&gt; k8s Application</p>"},{"location":"usage/routing-to-cluster-apps/#provisioning-dns-records","title":"Provisioning DNS Records","text":""},{"location":"usage/routing-to-cluster-apps/#automatic-dns-external-dns","title":"Automatic DNS (external-dns)","text":"<ul> <li>We have the <code>external-dns</code> operator installed in the cluster, which   can automatically provision Route53 domains on request.</li> <li>Authentication with Route53 is done via static credentials:<ul> <li>This means we simply use an IAM User with assigned access/secret   key pair.</li> <li>Typically IAM Roles for Service Accounts are recommended instead,   for generation or automatic temporary credentials.</li> <li>We use static credentials for (1) simplicity (2) less reliance   on AWS specific config, allowing for easier migration away if   needed.</li> <li>To improve security, we strictly limit IAM access for the user.</li> </ul> </li> </ul> <p>There is a great guide for doing this in the official external-dns docs, but the main steps are below.</p>"},{"location":"usage/routing-to-cluster-apps/#configuring-external-dns","title":"Configuring external-dns","text":"<ul> <li>Create policy.json</li> </ul> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:route53:::hostedzone/Z007708822IRWZYIAATO8\",\n        \"arn:aws:route53:::hostedzone/Z01079471L60VTHK00IFQ\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\",\n        \"route53:ListTagsForResource\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n</code></pre> <ul> <li>Create a policy from <code>policy.json</code>:</li> </ul> <pre><code>aws --profile admin iam create-policy --policy-name \"k8sExternalDnsRoute53\" --policy-document file://policy.json\n\n# example: arn:aws:iam::XXXXXXXXXXXX:policy/k8sExternalDnsRoute53\nexport POLICY_ARN=$(aws --profile admin iam list-policies \\\n --query 'Policies[?PolicyName==`k8sExternalDnsRoute53`].Arn' --output text)\n</code></pre> <ul> <li>Create users with the attached policy:</li> </ul> <pre><code># create IAM user\naws --profile admin iam create-user --user-name \"externaldns\"\n\n# attach policy arn created earlier to IAM user\naws --profile admin iam attach-user-policy --user-name \"externaldns\" --policy-arn $POLICY_ARN\n</code></pre> <ul> <li>Create security creds:</li> </ul> <pre><code>SECRET_ACCESS_KEY=$(aws --profile admin iam create-access-key --user-name \"externaldns\")\nACCESS_KEY_ID=$(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.AccessKeyId')\n\ncat &lt;&lt;-EOF &gt; credentials\n[default]\naws_access_key_id = $(echo $ACCESS_KEY_ID)\naws_secret_access_key = $(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.SecretAccessKey')\nEOF\n</code></pre> <ul> <li>Create a sealed secret for storing in this repo:</li> </ul> <pre><code>kubectl create secret generic external-dns-aws-creds \\\n  --from-file=credentials=./credentials \\\n  --namespace kube-system \\\n  --dry-run=client -o yaml &gt; secret.yaml\n\nkubeseal -f secret.yaml -w sealed-secret.yaml\n# Move the file to apps/external-dns/sealed-secret.yaml\n# The secret.yaml can be discarded\n</code></pre>"},{"location":"usage/routing-to-cluster-apps/#using-external-dns","title":"Using external-dns","text":"<ul> <li>It' works by adding annotations such as:   <code>external-dns.alpha.kubernetes.io/hostname: api.imagery.hotosm.org</code></li> <li>The annotation is picked up when an Ingress is made, and the   DNS entry is automatically made in Route53.</li> </ul>"},{"location":"usage/routing-to-cluster-apps/#updating-external-dns","title":"Updating external-dns","text":"<ul> <li>To update in future (e.g. update version), modify the file:   <code>apps/external-dns.yaml</code></li> <li>To add a new hosted zone:<ol> <li>Modify the IAM Policy to include the Zone ID.</li> <li>Modify the helm values to include the domain.</li> </ol> </li> </ul>"},{"location":"usage/routing-to-cluster-apps/#manual-dns","title":"Manual DNS","text":"<ul> <li>This shouldn't need to be done, but just for information   about how this works.</li> <li>Go to Route53 and create a new A record.</li> <li>Check 'Alias' and 'Alias to Application and Classic Load Balancer'.</li> <li>Set region to <code>us-east-1</code>.</li> <li>We are currently using 'classic' load balancer:   <code>dualstack.a1294ee7c4d2e41de88a0a5451b065b4-1435866857.us-east-1.elb.amazonaws.com.</code></li> </ul>"}]}